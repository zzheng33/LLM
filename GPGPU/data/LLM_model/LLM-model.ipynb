{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "863ec998-b23c-48b1-9ff2-e6ffcb94a411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM,AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d6fd05-b4ff-492c-aec3-3d08cddfb147",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# GPT-j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2af8c91e-1ac4-4c39-8465-b6f6ad616fa3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 02:48:35.952932: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-02 02:48:35.966106: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748832515.981849 3713085 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748832515.986606 3713085 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748832515.998639 3713085 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748832515.998652 3713085 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748832515.998654 3713085 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748832515.998655 3713085 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-02 02:48:36.002594: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of the model checkpoint at EleutherAI/gpt-j-6B were not used when initializing GPTJForCausalLM: ['transformer.h.0.attn.bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.10.attn.bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.12.attn.bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.13.attn.bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.14.attn.bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.15.attn.bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.16.attn.bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.17.attn.bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.18.attn.bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.19.attn.bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.2.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.20.attn.bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.21.attn.bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.22.attn.bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.23.attn.bias', 'transformer.h.23.attn.masked_bias', 'transformer.h.24.attn.bias', 'transformer.h.24.attn.masked_bias', 'transformer.h.25.attn.bias', 'transformer.h.25.attn.masked_bias', 'transformer.h.26.attn.bias', 'transformer.h.26.attn.masked_bias', 'transformer.h.27.attn.bias', 'transformer.h.27.attn.masked_bias', 'transformer.h.3.attn.bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.bias', 'transformer.h.9.attn.masked_bias']\n",
      "- This IS expected if you are initializing GPTJForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTJForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", torch_dtype=torch.float16, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fa55980-853c-4db9-8ed5-69d607893e06",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15011/15011 [00:00<00:00, 334566.32 examples/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When did Virgin Australia start operating?\n",
      "Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\n",
      "[Tokenization] 0.002 seconds\n",
      "[Generation] 3.047 seconds\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "# Pick a prompt from the dataset\n",
    "example = dataset[0]\n",
    "prompt = example[\"instruction\"] + \"\\n\" + example[\"context\"] if example[\"context\"] else example[\"instruction\"]\n",
    "print(prompt)\n",
    "\n",
    "\n",
    "# Time tokenization\n",
    "start = time.time()\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "tokenization_time = time.time() - start\n",
    "print(f\"[Tokenization] {tokenization_time:.3f} seconds\")\n",
    "# print(f\"Tokenized input: {inputs['input_ids'].shape[-1]} tokens\")\n",
    "\n",
    "# Time generation\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=False\n",
    "    )\n",
    "torch.cuda.synchronize()\n",
    "generation_time = time.time() - start\n",
    "print(f\"[Generation] {generation_time:.3f} seconds\")\n",
    "\n",
    "# Decode output\n",
    "result = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "# print(\"\\n[Generated text]:\")\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffaae5ae-694d-4059-b5ae-1715b710cca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running batch inference on 1 prompts...\n",
      "[Tokenization] 0.001 seconds\n",
      "[Generation] 3.070 seconds\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "batch_size = 1\n",
    "batch = []\n",
    "for i in range(batch_size):\n",
    "    example = dataset[i]\n",
    "    if example[\"context\"]:\n",
    "        prompt = example[\"instruction\"] + \"\\n\" + example[\"context\"]\n",
    "    else:\n",
    "        prompt = example[\"instruction\"]\n",
    "    batch.append(prompt)\n",
    "\n",
    "\n",
    "print(f\"Running batch inference on {len(batch)} prompts...\")\n",
    "\n",
    "# Time tokenization\n",
    "start = time.time()\n",
    "inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048)\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "tokenization_time = time.time() - start\n",
    "print(f\"[Tokenization] {tokenization_time:.3f} seconds\")\n",
    "\n",
    "# Time generation\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=100,\n",
    "        do_sample=False\n",
    "    )\n",
    "torch.cuda.synchronize()\n",
    "generation_time = time.time() - start\n",
    "print(f\"[Generation] {generation_time:.3f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719f26d2-c57b-4276-856f-82ce4e48de9b",
   "metadata": {},
   "source": [
    "# Flan-T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93ea0f99-8938-4a85-ac33-2f5f58c0b49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 02:53:08.867779: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-02 02:53:08.880969: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748832788.896925 3713951 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748832788.901768 3713951 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748832788.913907 3713951 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748832788.913920 3713951 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748832788.913922 3713951 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748832788.913923 3713951 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-02 02:53:08.918377: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Fetching 2 files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:29<00:00, 14.82s/it]\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.09it/s]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"google/flan-t5-xl\"  # try flan-t5-large or flan-t5-base if you hit OOM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c6b19c1-481a-468e-ac81-79dc185c28b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "# example = dataset[0]\n",
    "# prompt = example[\"instruction\"]\n",
    "# if example[\"context\"]:\n",
    "#     prompt += \"\\n\" + example[\"context\"]\n",
    "\n",
    "\n",
    "prompt = (\n",
    "    \"Write a detailed scientific explanation of how general relativity and quantum field theory could be unified \"\n",
    "    \"into a theory of quantum gravity. Cover mathematical foundations, known challenges, and current research directions.\"\n",
    ")\n",
    "\n",
    "base_sentence = \"Describe the process of star formation in high-resolution astrophysical detail. \"\n",
    "repeat_count = 100  # try 10,000 for real scaling\n",
    "prompt = base_sentence * repeat_count\n",
    "print(f\"Prompt length: {len(prompt)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2eece3fa-0981-4bcd-8a0f-1828bb1bc29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tokenization] 0.002 s\n",
      "[Generation] 8.236 s\n",
      "Generated 316 tokens\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "tokenization_time = time.time() - start\n",
    "print(f\"[Tokenization] {tokenization_time:.3f} s\")\n",
    "\n",
    "# Generate long output\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=1024,       # Increase for longer generation time\n",
    "        do_sample=False            # Optional: set to True for diverse outputs\n",
    "    )\n",
    "torch.cuda.synchronize()\n",
    "generation_time = time.time() - start\n",
    "print(f\"[Generation] {generation_time:.3f} s\")\n",
    "print(f\"Generated {output.shape[-1]} tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df5de94-6158-4bd9-8003-29229b4eff8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
